"""
RAG con LangGraph usando LLM as Judge
Flujo: Retrieve â†’ Generate â†’ Judge â†’ Refine (si es mala) â†’ Retrieve â†’ Generate â†’ End
"""

from typing import TypedDict, Annotated, Literal
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.chat_models import init_chat_model
import operator
import json


class RAGState(TypedDict):
    """Estado del grafo RAG con validaciÃ³n"""
    question: str  # Pregunta original
    query: str  # Query refinada (puede cambiar)
    context: str  # Contexto recuperado
    generation: str  # Respuesta generada
    is_valid: bool  # Si la respuesta es vÃ¡lida segÃºn el judge
    refinement_attempts: Annotated[int, operator.add]  # Contador de intentos
    retrieval_attempts: Annotated[int, operator.add]  # Contador de retrieves


class RAGGraph:
    """
    Clase que encapsula la lÃ³gica del RAG con LangGraph y LLM as Judge
    """
    
    def __init__(self, embedding_generator, reranker, get_document_func):
        """
        Inicializar el grafo RAG
        
        Args:
            embedding_generator: Generador de embeddings para queries
            reranker: Reranker para ordenar resultados
            get_document_func: FunciÃ³n para obtener documentos de Milvus
        """
        self.embedding_generator = embedding_generator
        self.reranker = reranker
        self.get_document_func = get_document_func
        self.workflow = None
        self.app = None
    
    def retrieve_context(self, state: RAGState) -> RAGState:
        """
        Nodo 1: Recupera contexto de Milvus directamente
        """
        print(f"\n [RETRIEVE] Buscando contexto para: '{state['query']}'")
        
        try:
            # Generar embedding de la query
            query_vector = self.embedding_generator.get_query_embedding(text=state["query"])
            
            # Obtener documentos de Milvus
            results = self.get_document_func(
                query_vector=query_vector, 
                collection_name="documents_collection", 
                filter=""
            )
            
            # Reranking
            context = self.reranker.rerank(query=state["query"], document=[results])
            
            # Convertir a string si es necesario
            context = str(context) if not isinstance(context, str) else context
            
        except Exception as e:
            context = f"Error al recuperar contexto: {str(e)}"
            print(f"   Error: {str(e)}")
        
        return {
            **state,
            "context": context,
            "retrieval_attempts": 1
        }

    def generate_answer(self, state: RAGState) -> RAGState:
        """
        Nodo 2: Genera respuesta usando el contexto
        """
        print("\nâœï¸  [GENERATE] Generando respuesta...")
        
        model = init_chat_model("google_genai:gemini-2.5-flash-lite")
        
        generator_prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un asistente/profesor Ãºtil y conciso que ayuda a los usuarios a estudiar usando **Ãºnicamente** el PDF previamente subido como contexto.  

- Usa exclusivamente la informaciÃ³n presente en `{context}`.  
- **No inventes** informaciÃ³n. Si la respuesta no puede responderse con lo provisto, responde exactamente: "No puedo responder eso con la informaciÃ³n provista." y, si es posible, sugiere 1â€“2 acciones para obtener la respuesta.
- **No reveles razonamiento interno** (no escribir chain-of-thought). En lugar de eso, entrega:  
  1) **Respuesta** â€” respuesta clara y directa (1â€“3 frases);  
  2) **JustificaciÃ³n en base al texto** â€” hasta 3 bullets que indiquen quÃ© partes del `{context}` sustentan la respuesta;"""),
            ("human", "{question}")
        ])
        
        generator_chain = generator_prompt | model | StrOutputParser() ##chain
        
        generation = generator_chain.invoke({
            "context": state["context"],
            "question": state["question"]
        })
        
        return {
            **state,
            "generation": generation
        }

    def judge_answer(self, state: RAGState) -> RAGState:
        """
        Nodo 3: LLM as Judge - EvalÃºa si la respuesta es buena usando un modelo mÃ¡s grande
        """
        print("\nğŸ§‘â€âš–ï¸  [JUDGE] Evaluando calidad de la respuesta...")
        
        # Usar un modelo mÃ¡s grande para juzgar
        judge_model = init_chat_model("google_genai:gemini-2-pro")
        
        judge_prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un juez experto que evalÃºa si una respuesta es de buena calidad.

Analiza si:
1. La respuesta estÃ¡ fundamentada en el contexto proporcionado
2. La respuesta es precisa y relevante a la pregunta
3. La respuesta no contiene informaciÃ³n inventada fuera del contexto
4. Si dice "No puedo responder...", determina si es una respuesta vÃ¡lida a falta de informaciÃ³n

Responde con un JSON con este formato EXACTO:
{
    "is_valid": true/false,
    "reasoning": "explicaciÃ³n breve"
}

NO agregues mÃ¡s texto, SOLO el JSON."""),
            ("human", """Pregunta: {question}

Contexto: {context}

Respuesta: {generation}

Â¿Es una buena respuesta?""")
        ])
        
        try:
            result = judge_model.invoke(judge_prompt.format(
                question=state["question"],
                context=state["context"],
                generation=state["generation"]
            ))
            
            # Parsear respuesta JSON
            result_json = json.loads(result)
            is_valid = result_json.get("is_valid", False)
            reasoning = result_json.get("reasoning", "")
            
            print(f"  Veredicto: {' VÃ¡lida' if is_valid else ' InvÃ¡lida'} - {reasoning}")
        except Exception as e:
            print(f"  Error en evaluaciÃ³n: {str(e)}")
            is_valid = True  # Si falla el judge, aceptar la respuesta
        
        return {
            **state,
            "is_valid": is_valid
        }

    def refine_query(self, state: RAGState) -> RAGState:
        """
        Nodo 4: Refina la query cuando la respuesta es invÃ¡lida
        """
        print("\nğŸ”§ [REFINE] Refinando query para obtener mejor contexto...")
        
        model = init_chat_model("google_genai:gemini-2.5-flash-lite")
        
        refiner_prompt = ChatPromptTemplate.from_messages([
            ("system", """Eres un experto en reformular preguntas para mejorar la recuperaciÃ³n de informaciÃ³n.

La pregunta original no obtuvo una respuesta satisfactoria. ReformÃºlala usando:
- SinÃ³nimos mÃ¡s especÃ­ficos
- TÃ©rminos tÃ©cnicos relevantes
- Diferentes Ã¡ngulos de la pregunta
- DescomposiciÃ³n en sub-preguntas si es necesario

Responde SOLO con la pregunta reformulada, sin explicaciones adicionales."""),
            ("human", "Pregunta original: {question}\n\nRespuesta insatisfactoria: {generation}")
        ])
        
        refiner_chain = refiner_prompt | model | StrOutputParser()
        
        refined_query = refiner_chain.invoke({
            "question": state["question"],
            "generation": state["generation"]
        })
        
        print(f"  Query refinada: '{refined_query}'")
        
        return {
            **state,
            "query": refined_query,
            "refinement_attempts": 1
        }

    def should_refine(self, state: RAGState) -> Literal["refine", "end"]:
        """
        Nodo de decisiÃ³n: Â¿Refinar y reintentar o terminar?
        """
        max_refinements = 2
        
        if not state["is_valid"] and state["refinement_attempts"] < max_refinements:
            print(f"\n [DECISION] Refinando (Intento {state['refinement_attempts']}/{max_refinements})")
            return "refine"
        else:
            if state["is_valid"]:
                print("\n [DECISION] Respuesta validada, terminando")
            else:
                print(f"\n [DECISION] MÃ¡ximos intentos de refinamiento alcanzados, terminando")
            return "end"

    def build(self):
        """
        Construye y compila el grafo
        """
        self.workflow = StateGraph(RAGState)
        
        # Agregar nodos (ahora son mÃ©todos de la clase)
        self.workflow.add_node("retrieve", self.retrieve_context)
        self.workflow.add_node("generate", self.generate_answer)
        self.workflow.add_node("judge", self.judge_answer)
        self.workflow.add_node("refine", self.refine_query)
        
        # Definir flujo
        self.workflow.set_entry_point("retrieve")
        self.workflow.add_edge("retrieve", "generate")
        self.workflow.add_edge("generate", "judge")
        self.workflow.add_conditional_edges(
            "judge",
            self.should_refine,
            {
                "refine": "refine",
                "end": END
            }
        )
        self.workflow.add_edge("refine", "retrieve")
        
        # Compilar
        self.app = self.workflow.compile()
        
        return self.app
    
    def invoke(self, initial_state: RAGState):
        """
        Invoca el grafo con un estado inicial
        """
        if self.app is None:
            self.build()
        
        return self.app.invoke(initial_state)


# Patron builder, crea el el objeto RAGGraph y construye el grafo (objeto) dentro de el atributo workflow
def create_rag_graph(embedding_generator, reranker, get_document_func):
    """
    Factory function que crea una instancia de RAGGraph y construye el grafo
    
    Args:
        embedding_generator: Generador de embeddings
        reranker: Reranker de resultados
        get_document_func: FunciÃ³n para obtener documentos de Milvus
    """
    rag_graph = RAGGraph(embedding_generator, reranker, get_document_func)
    rag_graph.build()
    return rag_graph
